\documentclass[12pt]{article}
\textwidth 15.5cm \oddsidemargin 0cm \topmargin -1cm \textheight
24cm \footskip 1.5cm \usepackage{epsfig}
\usepackage{amsmath,graphicx,psfrag,pstcol}
\usepackage{hyperref}
\def\n{\noindent}
\def\u{\underline}
\def\hs{\hspace}
\usepackage{graphicx}
\graphicspath{{../Figures/}}
\newcommand{\thrfor}{.^{\displaystyle .} .}
\newcommand{\bvec}[1]{{\bf #1}}
\usepackage{listings} % 核心宏包：代码插入
\usepackage{xcolor}

\begin{document}

\noindent
\rule{15.7cm}{0.5mm}


\begin{center}
{\bf ENGINEERING TRIPOS PART IIA}
\end{center}
\vspace{0.5cm} {\bf EIETL \hfill MODULE EXPERIMENT 3F3}
\vspace{0.5cm}
\begin{center}
{\bf RANDOM VARIABLES and RANDOM NUMBER GENERATION\\
Short  Report\\\hfill \\
Name: Yongqing Jiang\\\hfill\\
College: Peterhouse\\\hfill\\
crsid: yj375
}
\end{center}
\rule{15.7cm}{0.5mm}

\vspace*{1cm}

\begin{enumerate}
\item {\MakeUppercase{\textbf{Uniform and normal random variables.}}}

{\bf Histogram of Gaussian random numbers
overlaid on exact Gaussian curve (scaled):}

\begin{figure}[htbp] 
    \centering  
    \includegraphics[width=1\textwidth]{Figure_1.png}   
\end{figure}

\newpage

{\bf Histogram of Uniform random numbers overlaid 
on exact Uniform curve (scaled):}

\begin{figure}[htbp] 
    \centering  
    \includegraphics[width=1\textwidth]{Figure_2.png}   
\end{figure}



{\bf Kernel density estimate for Gaussian 
random numbers overlaid on exact Gaussian curve:}

\begin{figure}[htbp] 
    \centering  
    \includegraphics[width=1\textwidth]{Figure_3.png}   
\end{figure}


{\bf Kernel density estimate for Uniform random 
numbers overlaid on exact Gaussian curve:}

\begin{figure}[htbp] 
    \centering  
    \includegraphics[width=1\textwidth]{Figure_4.png}   
\end{figure}

\newpage

{\bf Comment on the advantages and disadvantages of the kernel density 
method compared with the histogram method for 
estimation of a probability density from random samples:}


According to the plots, kernel smoothing
function works well with Gaussian random numbers. It is
very close to the exact Gaussian curve. However, it does not
perform well when it comes to uniform random numbers.
We can see that the smoothing function of uniform random
numbers is close to a Gaussian curve. This is due to large 
value of width applied in the kernel smoothing function.
A large value of width underfits the random numbers and makes
the smoothing function similar to the kernel function itself,
which is a standard Gaussian distribution in this context.

If the width is set to smaller values, for example, 0.1. 
The smoothing function provides a good approximation to the 
expected probability density function, which is a constant from
0 to 1. But it overfits the data, the density curve is no longer
a constant value as expected. It can be seen that fine-tuning
on the width is much harder for uniform random numbers.

To sum up, kernel density method works well with Gaussian 
random numbers, and becomes harder to fine-tune the parameter
to fit uniform random numbers.

{\bf Theoretical mean and standard deviation 
calculation for uniform density as a function of $N$:}

Suppose a set of N independent random samples are taken
from a uniform distribution:

\begin{equation*}
    x^{(i)}  \sim p_U(x) = \frac{1}{N}, i = 1,...,N
\end{equation*}
The probability that a sample $x^{(j)}$ lies within
a particular bin of the histogram is:
\begin{equation*}
    p_j = \int^{c_j + \delta/2}_{c_j-\delta/2} p(x) dx = \frac{\delta}{N}
\end{equation*}
in which $\delta$ represents the size of a bin.
\newline
Thus, the mean of the count data in bin $j$ is 
$Np_j = \delta$.
The variance is $Np_j(1-p_j) = \delta (1-\frac{\delta}{N})$



{\bf Explain behaviour as $N$ becomes large:}
Assuming a constant bin size, it is clear that
the mean is not affected by N, and the variance
decreases as N increases.


{\bf Plot of histograms for $N=100$,  
$N=1000$ and $N=10000$ with theoretical mean  
and $\pm 3$ standard deviation lines:}

\begin{figure}[htbp] 
    \centering  
    \includegraphics[width=1\textwidth]{Figure_5.png}   
\end{figure}


{\bf Are your histogram results consistent 
with the multinomial distribution theory? }
For the case that N = 1000 and 10000, the histogram results
are consistent with the multinomial distribution theory.
Considering the counts of each bin as a random variable, 
its distribution approaches to Gaussian distribution 
as $N \rightarrow \infty$
according to Central Limit Theorem.
\newline
For a Gaussian distribution, 99\% of random variable lies within
the interval $(\mu - 3\sigma, \mu+3\sigma)$. From the plots that
N = 1000 and N = 10000, we can see that it is true.
\newline
However, the plot of N = 100 gives a negative value of $\mu - 3\sigma$.
Since $\sigma^2 = \delta(1-\frac{\delta}{N})$, a small N gives a 
large $\sigma$. This implies that histogram estimatio is not
suitable for a set of random numbers with a small size.



\item \MakeUppercase{{\textbf{Functions of random variables}}}

{\bf For normally distributed ${\cal N}(x|0,1)$ 
random variables, take $y=f(x)=ax+b$. 
Calculate $p(y)$ using the Jacobian formula:}

The Jacobian formula provided by lab handout:
\begin{equation*}
    p(y) = \frac{p(x)}{|dy/dx|}|_{x = f^{-1}(y)}
\end{equation*}







Explain how this is linked to the general normal density with non-zero mean and non-unity variance:


{\em Text/maths answer  here}
\vspace{3in}



 Verify this formula by transforming a large collection of random samples $x^{(i)}$ to give $y^{(i)}=f(x^{(i)})$, histogramming the resulting $y$ samples, and overlaying a plot of your formula calculated using the Jacobian:


{\em Include your graphic here}
\vspace{3in}




Now take $p(x)={\cal N}(x|0,1)$ and $f(x)=x^2$. Calculate $p(y)$ using the Jacobian formula:

{\em Text/maths answer  here}
\vspace{3in}




 Verify your result by histogramming of transformed random samples:


{\em Include your graphic here}
\vspace{3in}


\item{\bf Inverse CDF method} 



Calculate the CDF and the inverse CDF for the exponential distribution: 

The pdf of an exponential distribution Y with mean 1 is:
\begin{equation*}
    f_Y(y) = e^{-y} 
\end{equation*}
The corresponding cdf is found by integration:
\begin{equation*}
    F_Y(y) = \int_{0}^{y} f_Y(t)dt = \int_0^y e^{-t}dt = 1 - e^{-y}
\end{equation*}
The inverse of this function is found by: $x = F_Y(y), y = F^{-1}_Y(x)$.
\begin{equation*}
    F^{-1}_Y(x) = -\ln{(1-x)} 
\end{equation*}

Matlab/Python code for inverse CDF method for generating samples from the exponential distribution:


\begin{lstlisting}[language=python, caption={MATLAB}]
x_data = np.random.uniform(0,1,1000)
y_data = -np.log(1-x_data)

y = np.linspace(0,5,1000)
exp_theoretical = np.e**(-y)
\end{lstlisting}



Plot histograms/ kernel density estimates and overlay them on the desired exponential density:


{\em Include your graphic here}
\vspace{3in}

\item {\bf Simulation from a `non-standard'  density.}

Matlab/Python code to generate $N$ random numbers drawn from the distribution of $X$:
\vspace{3in}

Plot some histogram density estimates with $\alpha=0.5,\,1.5$ and several values of $\beta$. 

\vspace{3in}

Hence comment on the interpretation of the parameters $\alpha$ and $\beta$.

\end{enumerate}

\appendix

\section{Link to worked files}

The worked python files are uploaded to a 
repository which can be found at:
\url{https://github.com/OliverJiang2025/3F3_lab.git}




\end{document}

